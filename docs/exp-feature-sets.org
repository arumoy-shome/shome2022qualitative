#+title: Experiment with Feature Sets
#+date: [2022-10-17 Mon]
#+options: toc:t
#+toc: tables
#+html_head: <link rel="stylesheet" href="main.css">
#+property: header-args:python :session *sh22qual* :exports both :eval no-export

This document analyses the =data/exp-feature-sets-*-50.csv= datasets
ie. results from the feature sets experiments.

* Init

#+begin_src python :results silent
  import os
  import sys
  import pandas as pd
  import numpy as np
  import seaborn as sns
  import matplotlib.pyplot as plt

  ROOTDIR = os.path.abspath(os.path.join(os.getcwd(), ".."))
  DATADIR = os.path.join(ROOTDIR, "data")
  sys.path.insert(0, ROOTDIR)
  pd.set_option('display.max_columns', None)
  pd.set_option('display.max_colwidth', None)
#+end_src

* EDA
Lets analyse the data obtained from 50 iterations of the experiment.
First, we need to combine the datasets into a single dataframe.

#+begin_src python
  import glob

  frames = [pd.read_csv(frame) for frame in glob.glob(os.path.join(DATADIR, "*-50.csv"))]

  data = pd.concat(frames)
  data.shape
#+end_src

#+RESULTS:
| 93750 | 25 |

#+begin_src python
  data.dtypes
#+end_src

#+RESULTS:
#+begin_example
num_negatives                    float64
theil_index                      float64
accuracy                         float64
FPR                              float64
average_abs_odds_difference      float64
disparate_impact                 float64
model                             object
FN                               float64
TNR                              float64
protected                         object
FP                               float64
base_rate                        float64
TPR                              float64
num_positives                    float64
true_positive_rate_difference    float64
FNR                              float64
TP                               float64
statistical_parity_difference    float64
PPV                              float64
f1                               float64
num_features                       int64
privileged                        object
iteration                          int64
TN                               float64
dataset_label                     object
dtype: object
#+end_example

* Disparate Impact & Statistical Parity Difference [0/2]
In this section we take a closer look at the disparate impact &
statistical parity difference metrics. We start with these two since
we have both data & model variants for these metrics.

#+begin_src python
  # fairness metrics are calculated without conditioning on any (un)privileged group
  metrics = data[data["privileged"] == "None"]
  metrics = metrics[[
      "disparate_impact",
      "statistical_parity_difference",
      "iteration",
      "model",
      "protected",
      "num_features",
      "dataset_label"
  ]]

  metrics
#+end_src

#+RESULTS:
#+begin_example
       disparate_impact  statistical_parity_difference  iteration  \
0              0.380463                      -0.190953          0   
3              0.325643                      -0.176814          0   
6              0.425396                      -0.167885          0   
9              0.322013                      -0.170099          0   
12             0.360431                      -0.176439          0   
...                 ...                            ...        ...   
29985          0.456837                      -0.138794         49   
29988          0.272496                      -0.112363         49   
29991          0.528534                      -0.108183         49   
29994          0.263804                      -0.117350         49   
29997          0.384522                      -0.081692         49   

                        model protected  num_features dataset_label  
0                        None       sex            11         adult  
3          logisticregression       sex            11         adult  
6      decisiontreeclassifier       sex            11         adult  
9          adaboostclassifier       sex            11         adult  
12     randomforestclassifier       sex            11         adult  
...                       ...       ...           ...           ...  
29985                    None      RACE             3          meps  
29988      logisticregression      RACE             3          meps  
29991  decisiontreeclassifier      RACE             3          meps  
29994      adaboostclassifier      RACE             3          meps  
29997  randomforestclassifier      RACE             3          meps  

[31250 rows x 7 columns]
#+end_example

Lets preprocess the data next prior to analysis. zhang2021ignorance
took the absolute value for all fairness metrics. For disparate impact
zhang took the distance of the metric to one & then normalised the
value between [0, 1]. We also lowercase the values in the =protected=
column while we are are at it.

#+begin_src python :results silent
  cols = ["disparate_impact", "statistical_parity_difference"]
  metrics["protected"] = metrics["protected"].str.lower()
  metrics["disparate_impact"] = metrics["disparate_impact"] - 1
  metrics[cols] = metrics[cols].abs()

  from sklearn.preprocessing import MinMaxScaler

  scaler = MinMaxScaler()
  metrics["disparate_impact"] = scaler.fit_transform(
      metrics["disparate_impact"].values.reshape(-1, 1)
  ).ravel()

#+end_src

Lets create a simple lineplot to observe the general trend of the
fairness metrics across various feature sets. Since different datasets
have different protected attributes, we need to create separate
figures for each dataset.

#+begin_src python :results silent :exports none
  dataset_labels = metrics["dataset_label"].unique().tolist()
  cols = ["disparate_impact", "statistical_parity_difference"]

  for dataset_label in dataset_labels:
      df = metrics[metrics["dataset_label"] == dataset_label]
      name = "lineplot--exp-feature-sets--{}--di-spd.svg".format(dataset_label)
      protected = df["protected"].unique().tolist()
      fig, axs = plt.subplots(
          nrows=len(protected),
          ncols=len(cols),
          figsize=(5*len(cols),5*len(protected)),
          sharey=True,
      )

      for row, p in enumerate(protected):
          for col, metric in enumerate(cols):
              ax=axs[row,col] if len(protected) > 1 else axs[col]
              ax.set_title("protected: {}".format(p))
              ax.set_xlabel("num_features")
              ax.set_ylabel(metric)
              sns.lineplot(
                  data=df,
                  y=metric,
                  x="num_features",
                  hue="model",
                  style="model",
                  ax=ax,
              )

      fig.tight_layout()
      fig.savefig(name, format="svg")
#+end_src

#+caption: Lineplot DI & SPD Adult
#+name: fig-adult-lineplot-di-spd
[[file:lineplot--exp-feature-sets--adult--di-spd.svg]]

#+caption: Lineplot DI & SPD Compas
#+name: fig-compas-lineplot-di-spd
[[file:lineplot--exp-feature-sets--compas--di-spd.svg]]

#+caption: Lineplot DI & SPD Bank
#+name: fig-bank-lineplot-di-spd
[[file:lineplot--exp-feature-sets--bank--di-spd.svg]]

#+caption: Lineplot DI & SPD German
#+name: fig-german-lineplot-di-spd
[[file:lineplot--exp-feature-sets--german--di-spd.svg]]

#+caption: Lineplot DI & SPD Meps
#+name: fig-meps-lineplot-di-spd
[[file:lineplot--exp-feature-sets--meps--di-spd.svg]]

** TODO conduct analysis
- [ ] focus on each dataset
- [ ] widen across all datasets

** TODO High level observations [0/1]
- generally speaking, in some dataset-protected-model combinations I
  see that increasing feature set size improves fairness
- the data variants generally stay constant while model variants show
  improve in fairness as feature set increases
- some models (like decisiontreeclassifier) is relatively stable while
  other models (adaboost) improve a lot
- models for some dataset-protected combinations show significant
  improvements (compas-{sex,face} for instance) across both metrics
  while others (like bank-age) show significant improvement only in
  one metric; but this could simply be due to the mathematical formula
  of the metric (SPD is a difference; less variance?)
- no discernable improvements in some dataset-protected pairs
  (german-{age,sex} & meps-race)
- [ ] my expectation was that the data variants would be constant
  (straight line) since we use the same test split across all
  iterations; unless there is a bug in the code?

Lets also look at the distribution of the fairness metrics across
feature set sizes using boxplots.

** TODO analyse boxplot results [0/0]
- 
#+begin_src python :results silent :exports none
  dataset_labels = metrics["dataset_label"].unique().tolist()
  cols = ["disparate_impact", "statistical_parity_difference"]

  for dataset_label in dataset_labels:
      df = metrics[metrics["dataset_label"] == dataset_label]
      name = "boxplot--exp-feature-sets--{}--di-spd.svg".format(dataset_label)
      protected = df["protected"].unique().tolist()
      fig, axs = plt.subplots(
          nrows=len(protected),
          ncols=len(cols),
          figsize=(10*len(cols),5*len(protected)),
          sharey=True,
      )

      for row, p in enumerate(protected):
          for col, metric in enumerate(cols):
              ax=axs[row,col] if len(protected) > 1 else axs[col]
              ax.set_title("protected: {}".format(p))
              ax.set_xlabel("num_features")
              ax.set_ylabel(metric)
              sns.boxplot(
                  data=df,
                  y=metric,
                  x="num_features",
                  hue="model",
                  dodge=True,
                  ax=ax,
              )

      fig.tight_layout()
      fig.savefig(name, format="svg")
#+end_src

#+caption: Boxplot DI & SPD Adult
#+name: fig-adult-boxplot-di-spd
[[file:boxplot--exp-feature-sets--adult--di-spd.svg]]

#+caption: Boxplot DI & SPD Compas
#+name: fig-compas-boxplot-di-spd
[[file:boxplot--exp-feature-sets--compas--di-spd.svg]]

#+caption: Boxplot DI & SPD Bank
#+name: fig-bank-boxplot-di-spd
[[file:boxplot--exp-feature-sets--bank--di-spd.svg]]

#+caption: Boxplot DI & SPD German
#+name: fig-german-boxplot-di-spd
[[file:boxplot--exp-feature-sets--german--di-spd.svg]]

#+caption: Boxplot DI & SPD Meps
#+name: fig-meps-boxplot-di-spd
[[file:boxplot--exp-feature-sets--meps--di-spd.svg]]

*** TODO add violinplots
So we can check the distribution within the iterations.

** Relationship between data & model variant [0/0]
In this section we want to validate that the data & model metrics are
related to one another. We employ two types of tests:
1. Correlation between data & model variants
2. Fitting a linear regression model between data & model variants

Lets create heatmaps of the correlations between the data & model
variants of the fairness metrics.

#+begin_src python :results silent :exports none
  dataset_labels = metrics["dataset_label"].unique().tolist()
  cols = ["disparate_impact", "statistical_parity_difference"]
  models = metrics["model"].unique().tolist()
  models.remove("None")

  for dataset_label in dataset_labels:
      df = metrics[metrics["dataset_label"] == dataset_label]
      name = "heatmap--exp-feature-sets--{}--di-spd.svg".format(dataset_label)
      protected = df["protected"].unique().tolist()
      num_features = df["num_features"].unique().tolist()
      num_features.sort()         # ascending order
      fig, axs = plt.subplots(
          nrows=len(protected),
          ncols=len(cols),
          figsize=(5*len(cols),5*len(protected)),
          sharey=True,
      )

      for row, p in enumerate(protected):
          _df = df[df["protected"] == p]
          for col, metric in enumerate(cols):
              frame = []
              # this is a crappy implementation; next loop can be put
              # outside the above loop, but then it makes creating the
              # figures a bit more tricky...
              for n in num_features:
                  __df = _df[_df["num_features"] == n]
                  pivot = __df.pivot(
                      index="iteration",
                      columns="model",
                      values="disparate_impact"
                  )
                  frame.append(pivot)

              frame = pd.concat(frame)
              ax=axs[row,col] if len(protected) > 1 else axs[col]
              ax.set_title("protected: {} metric: {}".format(p, metric))
              corr = frame.corr()
              mask = np.zeros_like(corr)
              mask[np.triu_indices_from(mask)] = True
              sns.heatmap(
                  data=corr,
                  mask=mask,
                  square=True,
                  ax=ax,
              )
      fig.tight_layout()
      fig.savefig(name, format="svg")
#+end_src

#+caption: Heatmap DI & SPD Adult
#+name: fig-adult-heatmap-di-spd
[[file:heatmap--exp-feature-sets--adult--di-spd.svg]]

#+caption: Heatmap DI & SPD Compas
#+name: fig-compas-heatmap-di-spd
[[file:heatmap--exp-feature-sets--compas--di-spd.svg]]

#+caption: Heatmap DI & SPD Bank
#+name: fig-bank-heatmap-di-spd
[[file:heatmap--exp-feature-sets--bank--di-spd.svg]]

#+caption: Heatmap DI & SPD German
#+name: fig-german-heatmap-di-spd
[[file:heatmap--exp-feature-sets--german--di-spd.svg]]

#+caption: Heatmap DI & SPD Meps
#+name: fig-meps-heatmap-di-spd
[[file:heatmap--exp-feature-sets--meps--di-spd.svg]]

+ [ ] analyse correlation heatmaps within datasets
+ [ ] then generalise results across datasets
+ [ ] currently we are generalising over all feature sets, should we
  be looking per feature set?
+ in general, I see that for certain dataset-protected-model
  combinations, the data & model variants are correlated.

Next, we want to fit a linear regression model on the data & model
variants. The data needs some manipulation to make it fit for further
visualisations. We want the following columns:
1. x: the data variant of fairness metrics (float)
2. y: the model variant of fairness metrics (float)
3. num_features: the number of features used (int)
4. model: the model used for the y value
5. protected: the name of the protected attribute
6. metric: name of the fairness metric
7. dataset_label: name of the dataset

#+begin_src python :exports both
  dataset_labels = metrics["dataset_label"].unique().tolist()
  cols = ["disparate_impact", "statistical_parity_difference"]
  models = metrics["model"].unique().tolist()
  models.remove("None")
  frame = []

  for dataset_label in dataset_labels:
      df = metrics[metrics["dataset_label"] == dataset_label]
      protected = df["protected"].unique().tolist()
      num_features = df["num_features"].unique().tolist()

      _frames = []
      for p in protected:
          _df = df[df["protected"] == p]
          for metric in cols:
              _pivots = []
              for n in num_features:
                  __df = _df[_df["num_features"] == n]
                  pivot = __df.pivot(
                      index="iteration",
                      columns="model",
                      values="disparate_impact"
                  )
                  pivot["num_features"] = n
                  _pivots.append(pivot)

              pivoted = pd.concat(_pivots)

              _chunks = []
              for x, y in zip(["None"]*len(models), models):
                  _chunk = pivoted[[x,y,"num_features"]]
                  _chunk = _chunk.rename(columns={x: "x", y: "y"})
                  _chunk["dataset_label"] = dataset_label
                  _chunk["model"] = y
                  _chunk["metric"] = metric
                  _chunk["protected"] = p
                  _chunks.append(_chunk)

              chunked = pd.concat(_chunks)
              _frames.append(chunked)
      framed = pd.concat(_frames)
      frame.append(framed)
  frame = pd.concat(frame)
  frame

#+end_src

#+RESULTS:
#+begin_example
model             x         y  num_features dataset_label  \
iteration                                                   
0          0.204947  0.223081            11         adult   
1          0.204947  0.223081            11         adult   
2          0.204947  0.223081            11         adult   
3          0.204947  0.223081            11         adult   
4          0.204947  0.223081            11         adult   
...             ...       ...           ...           ...   
45         0.179681  0.197927             3          meps   
46         0.179681  0.204885             3          meps   
47         0.179681  0.198855             3          meps   
48         0.179681  0.197746             3          meps   
49         0.179681  0.203604             3          meps   

model                       model                         metric protected  
iteration                                                                   
0              logisticregression               disparate_impact       sex  
1              logisticregression               disparate_impact       sex  
2              logisticregression               disparate_impact       sex  
3              logisticregression               disparate_impact       sex  
4              logisticregression               disparate_impact       sex  
...                           ...                            ...       ...  
45         randomforestclassifier  statistical_parity_difference      race  
46         randomforestclassifier  statistical_parity_difference      race  
47         randomforestclassifier  statistical_parity_difference      race  
48         randomforestclassifier  statistical_parity_difference      race  
49         randomforestclassifier  statistical_parity_difference      race  

[50000 rows x 7 columns]
#+end_example


#+begin_src python :results silent :exports none
  for dataset_label in dataset_labels:
      name = "regplot--exp-feature-sets--{}--di-spd.svg".format(dataset_label)
      df = frame[frame["dataset_label"] == dataset_label]
      g = sns.lmplot(
          data=df,
          x="x",
          y="y",
          hue="model",
          markers=[".", "+", "x", "^"],
          units="num_features",
          col="metric",
          row="protected",
          sharex=False,
      )
      g.tight_layout()
      g.savefig(name, format="svg")

#+end_src

** TODO add scatterplot to lmplot

#+caption: Regplot DI & SPD Adult
#+name: fig-adult-regplot-di-spd
[[file:regplot--exp-feature-sets--adult--di-spd.svg]]

#+caption: Regplot DI & SPD Compas
#+name: fig-compas-regplot-di-spd
[[file:regplot--exp-feature-sets--compas--di-spd.svg]]

#+caption: Regplot DI & SPD Bank
#+name: fig-bank-regplot-di-spd
[[file:regplot--exp-feature-sets--bank--di-spd.svg]]

#+caption: Regplot DI & SPD German
#+name: fig-german-regplot-di-spd
[[file:regplot--exp-feature-sets--german--di-spd.svg]]

#+caption: Regplot DI & SPD Meps
#+name: fig-meps-regplot-di-spd
[[file:regplot--exp-feature-sets--meps--di-spd.svg]]

The scatterplots bring in a lot of noise, lets try with plotting with
the mean of the 50 iterations.

- need to understand how linear regression works; I have a feeling
  what we are plotting does not make statistical sense!

** TODO lineplot data vs. model over num_features
Is there a linear relationship between model & data variants when we
vary the feature sets?

- this will be computational heavy (the grid will grow as number of
  features increases!); refactor this notebook's code into python
  module next

* Base rate & others [0/2]
In this section we consider all 4 fairness metrics & try to find a
relationship with the base rate metric since this is the only data
metric fairness that may be generalisable to all model fairness
metrics.

- [ ] lineplot of baserate vs. other metrics; how do the conditioned
  base rates compare to other metrics?
- [ ] 

