#+title: Adult
#+author: Arumoy Shome
#+date: [2022-07-25 Mon]
#+property: header-args:python :session *sh21qual-adult* :exports both :eval never-export

In this file we analyse the results obtained from computing all
=BinaryLabelDatasetMetric= & =ClassificationMetric= for the adult
dataset with a =LinearRegression= model. The dataset contains two
protected attributes: sex & race. We consider both protected
attributes individually in our analysis.

The data analysed here is generated using =bin/data.py=.

* Init
In this section we perform some sanity checks, load the necessary
modules & the dataset.

#+begin_src python :results silent
  import pandas as pd
  import numpy as np
  pd.set_option('display.max_columns', None)
  pd.set_option('display.max_colwidth', None)
  pd.set_option('display.max_rows', None)

  import matplotlib
  matplotlib.use('Agg')           # non-interactive backend
  import matplotlib.pyplot as plt
  import seaborn as sns

  import os
  import sys
  ROOTDIR = os.path.abspath(os.path.join(os.getcwd(), '..'))
  DATADIR = os.path.join(ROOTDIR, 'data')

  sys.path.insert(0, ROOTDIR)
  from src import utils
#+end_src

#+begin_src python :results silent
  adult = pd.read_csv(os.path.join(DATADIR, 'data.csv'))
  adult = adult[adult['dataset'] == 'adult']
#+end_src

* Priliminary analysis
In this section we conduct some priliminary analysis of the dataset.

#+begin_src python
  adult
#+end_src

#+RESULTS:
#+begin_example
    GFP       FDR      TN subset  statistical_parity_difference  GFN  \
0   NaN       NaN     NaN   full                      -0.198901  NaN   
1   NaN       NaN     NaN   full                            NaN  NaN   
2   NaN       NaN     NaN   full                            NaN  NaN   
3   NaN       NaN     NaN   full                      -0.103959  NaN   
4   NaN       NaN     NaN   full                            NaN  NaN   
5   NaN       NaN     NaN   full                            NaN  NaN   
6   NaN       NaN     NaN  train                      -0.201944  NaN   
7   NaN       NaN     NaN  train                            NaN  NaN   
8   NaN       NaN     NaN  train                            NaN  NaN   
9   NaN       NaN     NaN  train                      -0.105242  NaN   
10  NaN       NaN     NaN  train                            NaN  NaN   
11  NaN       NaN     NaN  train                            NaN  NaN   
12  NaN       NaN     NaN   test                      -0.189774  NaN   
13  NaN       NaN     NaN   test                            NaN  NaN   
14  NaN       NaN     NaN   test                            NaN  NaN   
15  NaN       NaN     NaN   test                      -0.100076  NaN   
16  NaN       NaN     NaN   test                            NaN  NaN   
17  NaN       NaN     NaN   test                            NaN  NaN   
18  0.0  0.270132  7866.0   test                      -0.184484  0.0   
19  0.0  0.271792  4713.0   test                            NaN  0.0   
20  0.0  0.259016  3153.0   test                            NaN  0.0   
21  0.0  0.270132  7866.0   test                      -0.095887  0.0   
22  0.0  0.265923  6614.0   test                            NaN  0.0   
23  0.0  0.316327  1252.0   test                            NaN  0.0   
24  0.0  0.364010  7527.0   test                      -0.177995  0.0   
25  0.0  0.352627  4476.0   test                            NaN  0.0   
26  0.0  0.423888  3051.0   test                            NaN  0.0   
27  0.0  0.364010  7527.0   test                      -0.081235  0.0   
28  0.0  0.357380  6324.0   test                            NaN  0.0   
29  0.0  0.425287  1203.0   test                            NaN  0.0   

         FPR  theil_index     GTP  GFPR  GTPR       FNR       FOR  GTNR  \
0        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
1        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
2        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
3        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
4        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
5        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
6        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
7        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
8        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
9        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
10       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
11       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
12       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
13       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
14       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
15       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
16       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
17       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
18  0.074588     0.122473  2806.0   0.0   1.0  0.389522  0.122000   1.0   
19  0.105353          NaN  2365.0   0.0   1.0  0.371247  0.157038   1.0   
20  0.024443          NaN   441.0   0.0   1.0  0.487528  0.063836   1.0   
21  0.074588     0.122473  2806.0   0.0   1.0  0.389522  0.122000   1.0   
22  0.079599          NaN  2552.0   0.0   1.0  0.381270  0.128246   1.0   
23  0.047184          NaN   254.0   0.0   1.0  0.472441  0.087464   1.0   
24  0.114471     0.132559  2806.0   0.0   1.0  0.394155  0.128113   1.0   
25  0.150342          NaN  2365.0   0.0   1.0  0.385201  0.169111   1.0   
26  0.056002          NaN   441.0   0.0   1.0  0.442177  0.060074   1.0   
27  0.114471     0.132559  2806.0   0.0   1.0  0.394155  0.128113   1.0   
28  0.119955          NaN  2552.0   0.0   1.0  0.392633  0.136773   1.0   
29  0.084475          NaN   254.0   0.0   1.0  0.409449  0.079572   1.0   

         NPV       TPR       PPV      FN       TNR     GTN     FP  \
0        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
1        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
2        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
3        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
4        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
5        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
6        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
7        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
8        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
9        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
10       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
11       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
12       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
13       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
14       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
15       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
16       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
17       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
18  0.878000  0.610478  0.729868  1093.0  0.925412  8500.0  634.0   
19  0.842962  0.628753  0.728208   878.0  0.894647  5268.0  555.0   
20  0.936164  0.512472  0.740984   215.0  0.975557  3232.0   79.0   
21  0.878000  0.610478  0.729868  1093.0  0.925412  8500.0  634.0   
22  0.871754  0.618730  0.734077   973.0  0.920401  7186.0  572.0   
23  0.912536  0.527559  0.683673   120.0  0.952816  1314.0   62.0   
24  0.871887  0.605845  0.635990  1106.0  0.885529  8500.0  973.0   
25  0.830889  0.614799  0.647373   911.0  0.849658  5268.0  792.0   
26  0.939926  0.557823  0.576112   195.0  0.943998  3232.0  181.0   
27  0.871887  0.605845  0.635990  1106.0  0.885529  8500.0  973.0   
28  0.863227  0.607367  0.642620  1002.0  0.880045  7186.0  862.0   
29  0.920428  0.590551  0.574713   104.0  0.915525  1314.0  111.0   

                     model      TP  accuracy  num_positives  num_negatives  \
0                     None     NaN       NaN        11208.0        34014.0   
1                     None     NaN       NaN         9539.0        20988.0   
2                     None     NaN       NaN         1669.0        13026.0   
3                     None     NaN       NaN        11208.0        34014.0   
4                     None     NaN       NaN        10207.0        28696.0   
5                     None     NaN       NaN         1001.0         5318.0   
6                     None     NaN       NaN         8402.0        25514.0   
7                     None     NaN       NaN         7174.0        15720.0   
8                     None     NaN       NaN         1228.0         9794.0   
9                     None     NaN       NaN         8402.0        25514.0   
10                    None     NaN       NaN         7655.0        21510.0   
11                    None     NaN       NaN          747.0         4004.0   
12                    None     NaN       NaN         2806.0         8500.0   
13                    None     NaN       NaN         2365.0         5268.0   
14                    None     NaN       NaN          441.0         3232.0   
15                    None     NaN       NaN         2806.0         8500.0   
16                    None     NaN       NaN         2552.0         7186.0   
17                    None     NaN       NaN          254.0         1314.0   
18      logisticregression  1713.0  0.847249            NaN            NaN   
19      logisticregression  1487.0  0.812263            NaN            NaN   
20      logisticregression   226.0  0.919956            NaN            NaN   
21      logisticregression  1713.0  0.847249            NaN            NaN   
22      logisticregression  1579.0  0.841343            NaN            NaN   
23      logisticregression   134.0  0.883929            NaN            NaN   
24  decisiontreeclassifier  1700.0  0.816115            NaN            NaN   
25  decisiontreeclassifier  1454.0  0.776890            NaN            NaN   
26  decisiontreeclassifier   246.0  0.897631            NaN            NaN   
27  decisiontreeclassifier  1700.0  0.816115            NaN            NaN   
28  decisiontreeclassifier  1550.0  0.808585            NaN            NaN   
29  decisiontreeclassifier   150.0  0.862883            NaN            NaN   

    base_rate privileged  GFNR dataset protected  disparate_impact  
0    0.247844       None   NaN   adult       sex          0.363470  
1    0.312477       True   NaN   adult       sex               NaN  
2    0.113576      False   NaN   adult       sex               NaN  
3    0.247844       None   NaN   adult      race          0.603769  
4    0.262371       True   NaN   adult      race               NaN  
5    0.158411      False   NaN   adult      race               NaN  
6    0.247730       None   NaN   adult       sex          0.355548  
7    0.313357       True   NaN   adult       sex               NaN  
8    0.111414      False   NaN   adult       sex               NaN  
9    0.247730       None   NaN   adult      race          0.599035  
10   0.262472       True   NaN   adult      race               NaN  
11   0.157230      False   NaN   adult      race               NaN  
12   0.248187       None   NaN   adult       sex          0.387509  
13   0.309839       True   NaN   adult       sex               NaN  
14   0.120065      False   NaN   adult       sex               NaN  
15   0.248187       None   NaN   adult      race          0.618126  
16   0.262066       True   NaN   adult      race               NaN  
17   0.161990      False   NaN   adult      race               NaN  
18        NaN       None   0.0   adult       sex          0.310398  
19        NaN       True   0.0   adult       sex               NaN  
20        NaN      False   0.0   adult       sex               NaN  
21        NaN       None   0.0   adult      race          0.565900  
22        NaN       True   0.0   adult      race               NaN  
23        NaN      False   0.0   adult      race               NaN  
24        NaN       None   0.0   adult       sex          0.395087  
25        NaN       True   0.0   adult       sex               NaN  
26        NaN      False   0.0   adult       sex               NaN  
27        NaN       None   0.0   adult      race          0.672027  
28        NaN       True   0.0   adult      race               NaN  
29        NaN      False   0.0   adult      race               NaN  
#+end_example

#+begin_src python
  adult.shape
#+end_src

#+RESULTS:
| 30 | 32 |

#+begin_src python
  adult.dtypes
#+end_src

#+RESULTS:
#+begin_example
GFP                              float64
FDR                              float64
TN                               float64
subset                            object
statistical_parity_difference    float64
GFN                              float64
FPR                              float64
theil_index                      float64
GTP                              float64
GFPR                             float64
GTPR                             float64
FNR                              float64
FOR                              float64
GTNR                             float64
NPV                              float64
TPR                              float64
PPV                              float64
FN                               float64
TNR                              float64
GTN                              float64
FP                               float64
model                             object
TP                               float64
accuracy                         float64
num_positives                    float64
num_negatives                    float64
base_rate                        float64
privileged                        object
GFNR                             float64
dataset                           object
protected                         object
disparate_impact                 float64
dtype: object
#+end_example

#+begin_src python
  adult.describe(include='all')
#+end_src

#+RESULTS:
#+begin_example
         GFP        FDR           TN subset  statistical_parity_difference  \
count   12.0  12.000000    12.000000     30                      10.000000   
unique   NaN        NaN          NaN      3                            NaN   
top      NaN        NaN          NaN   test                            NaN   
freq     NaN        NaN          NaN     18                            NaN   
mean     0.0   0.328377  5131.000000    NaN                      -0.143950   
std      0.0   0.061294  2511.484132    NaN                       0.050056   
min      0.0   0.259016  1203.000000    NaN                      -0.201944   
25%      0.0   0.270132  3127.500000    NaN                      -0.188451   
50%      0.0   0.334477  5518.500000    NaN                      -0.141618   
75%      0.0   0.364010  7527.000000    NaN                      -0.101047   
max      0.0   0.425287  7866.000000    NaN                      -0.081235   

         GFN        FPR  theil_index          GTP  GFPR  GTPR        FNR  \
count   12.0  12.000000     4.000000    12.000000  12.0  12.0  12.000000   
unique   NaN        NaN          NaN          NaN   NaN   NaN        NaN   
top      NaN        NaN          NaN          NaN   NaN   NaN        NaN   
freq     NaN        NaN          NaN          NaN   NaN   NaN        NaN   
mean     0.0   0.087123     0.127516  1870.666667   0.0   1.0   0.409108   
std      0.0   0.035385     0.005823  1137.448127   0.0   0.0   0.037562   
min      0.0   0.024443     0.122473   254.000000   0.0   1.0   0.371247   
25%      0.0   0.069942     0.122473   441.000000   0.0   1.0   0.388442   
50%      0.0   0.082037     0.127516  2458.500000   0.0   1.0   0.393394   
75%      0.0   0.114471     0.132559  2806.000000   0.0   1.0   0.417631   
max      0.0   0.150342     0.132559  2806.000000   0.0   1.0   0.487528   

              FOR  GTNR        NPV        TPR        PPV           FN  \
count   12.000000  12.0  12.000000  12.000000  12.000000    12.000000   
unique        NaN   NaN        NaN        NaN        NaN          NaN   
top           NaN   NaN        NaN        NaN        NaN          NaN   
freq          NaN   NaN        NaN        NaN        NaN          NaN   
mean     0.115195   1.0   0.884805   0.590892   0.671623   733.000000   
std      0.034947   0.0   0.034947   0.037562   0.061294   431.625469   
min      0.060074   1.0   0.830889   0.512472   0.574713   104.000000   
25%      0.085491   1.0   0.869622   0.582369   0.635990   210.000000   
50%      0.125057   1.0   0.874943   0.606606   0.665523   942.000000   
75%      0.130378   1.0   0.914509   0.611558   0.729868  1093.000000   
max      0.169111   1.0   0.939926   0.628753   0.740984  1106.000000   

              TNR          GTN          FP model           TP   accuracy  \
count   12.000000    12.000000   12.000000    30    12.000000  12.000000   
unique        NaN          NaN         NaN     3          NaN        NaN   
top           NaN          NaN         NaN  None          NaN        NaN   
freq          NaN          NaN         NaN    18          NaN        NaN   
mean     0.912877  5666.666667  535.666667   NaN  1137.666667   0.844184   
std      0.035385  2808.952355  345.725675   NaN   706.488542   0.041500   
min      0.849658  1314.000000   62.000000   NaN   134.000000   0.776890   
25%      0.885529  3232.000000  163.500000   NaN   241.000000   0.815152   
50%      0.917963  6227.000000  603.000000   NaN  1518.500000   0.844296   
75%      0.930058  8500.000000  809.500000   NaN  1700.000000   0.868144   
max      0.975557  8500.000000  973.000000   NaN  1713.000000   0.919956   

        num_positives  num_negatives  base_rate privileged  GFNR dataset  \
count       18.000000      18.000000  18.000000         30  12.0      30   
unique            NaN            NaN        NaN          3   NaN       1   
top               NaN            NaN        NaN       None   NaN   adult   
freq              NaN            NaN        NaN         10   NaN      30   
mean      4981.333333   15117.333333   0.224044        NaN   0.0     NaN   
std       4094.371229   10905.894596   0.068296        NaN   0.0     NaN   
min        254.000000    1314.000000   0.111414        NaN   0.0     NaN   
25%       1338.250000    5785.000000   0.159306        NaN   0.0     NaN   
50%       2806.000000   11410.000000   0.247844        NaN   0.0     NaN   
75%       8402.000000   24513.000000   0.262294        NaN   0.0     NaN   
max      11208.000000   34014.000000   0.313357        NaN   0.0     NaN   

       protected  disparate_impact  
count         30         10.000000  
unique         2               NaN  
top          sex               NaN  
freq          15               NaN  
mean         NaN          0.487087  
std          NaN          0.135776  
min          NaN          0.310398  
25%          NaN          0.369479  
50%          NaN          0.480493  
75%          NaN          0.602585  
max          NaN          0.672027  
#+end_example

Each metric is calculated for 3 different subsets of the dataset
(=train=, =test= & =full=). Each metric may further be conditioned in
3 different manner as indicated by value in the =privileged= column.
=None= means the metric is calculated on the full dataset, =True=
means it is conditioned on the privileged group (ie. =sex= is 1 or
'Male' in our case) and =False= means it is conditioned on the
unprivileged group (=sex= is 0 or 'Female').

* Analysis of protected attribute =sex=
In this section we analyse the fairness metrics. The section is
further divided into logical subsections.

#+begin_src python
  data = adult[adult['protected'] == 'sex']
  data.shape
#+end_src

#+RESULTS:
| 15 | 32 |

#+begin_src python
  data
#+end_src

#+RESULTS:
#+begin_example
    GFP       FDR      TN subset  statistical_parity_difference  GFN  \
0   NaN       NaN     NaN   full                      -0.198901  NaN   
1   NaN       NaN     NaN   full                            NaN  NaN   
2   NaN       NaN     NaN   full                            NaN  NaN   
6   NaN       NaN     NaN  train                      -0.201944  NaN   
7   NaN       NaN     NaN  train                            NaN  NaN   
8   NaN       NaN     NaN  train                            NaN  NaN   
12  NaN       NaN     NaN   test                      -0.189774  NaN   
13  NaN       NaN     NaN   test                            NaN  NaN   
14  NaN       NaN     NaN   test                            NaN  NaN   
18  0.0  0.270132  7866.0   test                      -0.184484  0.0   
19  0.0  0.271792  4713.0   test                            NaN  0.0   
20  0.0  0.259016  3153.0   test                            NaN  0.0   
24  0.0  0.364010  7527.0   test                      -0.177995  0.0   
25  0.0  0.352627  4476.0   test                            NaN  0.0   
26  0.0  0.423888  3051.0   test                            NaN  0.0   

         FPR  theil_index     GTP  GFPR  GTPR       FNR       FOR  GTNR  \
0        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
1        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
2        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
6        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
7        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
8        NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
12       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
13       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
14       NaN          NaN     NaN   NaN   NaN       NaN       NaN   NaN   
18  0.074588     0.122473  2806.0   0.0   1.0  0.389522  0.122000   1.0   
19  0.105353          NaN  2365.0   0.0   1.0  0.371247  0.157038   1.0   
20  0.024443          NaN   441.0   0.0   1.0  0.487528  0.063836   1.0   
24  0.114471     0.132559  2806.0   0.0   1.0  0.394155  0.128113   1.0   
25  0.150342          NaN  2365.0   0.0   1.0  0.385201  0.169111   1.0   
26  0.056002          NaN   441.0   0.0   1.0  0.442177  0.060074   1.0   

         NPV       TPR       PPV      FN       TNR     GTN     FP  \
0        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
1        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
2        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
6        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
7        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
8        NaN       NaN       NaN     NaN       NaN     NaN    NaN   
12       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
13       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
14       NaN       NaN       NaN     NaN       NaN     NaN    NaN   
18  0.878000  0.610478  0.729868  1093.0  0.925412  8500.0  634.0   
19  0.842962  0.628753  0.728208   878.0  0.894647  5268.0  555.0   
20  0.936164  0.512472  0.740984   215.0  0.975557  3232.0   79.0   
24  0.871887  0.605845  0.635990  1106.0  0.885529  8500.0  973.0   
25  0.830889  0.614799  0.647373   911.0  0.849658  5268.0  792.0   
26  0.939926  0.557823  0.576112   195.0  0.943998  3232.0  181.0   

                     model      TP  accuracy  num_positives  num_negatives  \
0                     None     NaN       NaN        11208.0        34014.0   
1                     None     NaN       NaN         9539.0        20988.0   
2                     None     NaN       NaN         1669.0        13026.0   
6                     None     NaN       NaN         8402.0        25514.0   
7                     None     NaN       NaN         7174.0        15720.0   
8                     None     NaN       NaN         1228.0         9794.0   
12                    None     NaN       NaN         2806.0         8500.0   
13                    None     NaN       NaN         2365.0         5268.0   
14                    None     NaN       NaN          441.0         3232.0   
18      logisticregression  1713.0  0.847249            NaN            NaN   
19      logisticregression  1487.0  0.812263            NaN            NaN   
20      logisticregression   226.0  0.919956            NaN            NaN   
24  decisiontreeclassifier  1700.0  0.816115            NaN            NaN   
25  decisiontreeclassifier  1454.0  0.776890            NaN            NaN   
26  decisiontreeclassifier   246.0  0.897631            NaN            NaN   

    base_rate privileged  GFNR dataset protected  disparate_impact  
0    0.247844       None   NaN   adult       sex          0.363470  
1    0.312477       True   NaN   adult       sex               NaN  
2    0.113576      False   NaN   adult       sex               NaN  
6    0.247730       None   NaN   adult       sex          0.355548  
7    0.313357       True   NaN   adult       sex               NaN  
8    0.111414      False   NaN   adult       sex               NaN  
12   0.248187       None   NaN   adult       sex          0.387509  
13   0.309839       True   NaN   adult       sex               NaN  
14   0.120065      False   NaN   adult       sex               NaN  
18        NaN       None   0.0   adult       sex          0.310398  
19        NaN       True   0.0   adult       sex               NaN  
20        NaN      False   0.0   adult       sex               NaN  
24        NaN       None   0.0   adult       sex          0.395087  
25        NaN       True   0.0   adult       sex               NaN  
26        NaN      False   0.0   adult       sex               NaN  
#+end_example

** Analysis of =num_{positive,negative}=, =num_{true,false}_{positive,negative}= & =num_{true,false}_{positive_negative}_rate=
We start with the =num_positives=, =num_negatives= which are computed
only using the dataset.

#+begin_src python :results file
  name = 'adult_barplot_prot-sex_subset-all_num-pos-neg'

  fig, axs = plt.subplots(1, 2, sharey=True, figsize=(10, 5))

  sns.barplot(data=data,
	      y='num_positives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[0])

  sns.barplot(data=data,
	      y='num_negatives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[1])

  # label the bars with the value, taken from
  # <https://stackoverflow.com/a/68323374>
  for container in axs[0].containers:
      axs[0].bar_label(container)

  for container in axs[1].containers:
      axs[1].bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-sex_subset-all_num-pos-neg.png]]

We note that the number of examples for the negative class is far more
than the positive class. This imbalance exists across the various
subsets, this makes sense since we take a random sample from the full
dataset to construct the train & test subsets.

In both metrics, we have more examples from the privileged group. That
is, we have more examples where the sex is 'Male'.

Thus we have two separate biases that we are dealing with: first is
the imbalance in the =class= column and second is the imbalance in the
=sex= column.

Lets zoom into only the train subset. This is purely for convenience
of comparing the data & model metrics together (since all model
metrics are calculated using only the test set).

#+begin_src python :results file
  name = 'adult_barplot_prot-sex_subset-test_num-pos-neg'

  fig, axs = plt.subplots(1, 2, sharey=True, figsize=(10, 5))

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='num_positives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[0])

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='num_negatives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[1])

  # label the bars with the value, taken from
  # <https://stackoverflow.com/a/68323374>
  for container in axs[0].containers:
      axs[0].bar_label(container)

  for container in axs[1].containers:
      axs[1].bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-sex_subset-test_num-pos-neg.png]]

Lets look at the confusion matrices for the models next to understand
the biases in them. The confusion matrices come in two flavours: the
absolute & normalised versions.

*** model: logisticregression
Lets start with the logisticregression model first.

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-lr_cm'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['TN', 'FP', 'FN', 'TP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-lr_cm.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-lr_cm-rate'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['TNR', 'FPR', 'FNR', 'TPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-lr_cm-rate.png]]

The model does well with the negative class (~92% accuracy). It
doesn't do so well with the positive class (~61% accuracy) with a less
then idea false negative rate (~39%). This is expected since we have
more number of negative examples in the dataset.

The performance of the model remains some what similar across the
conditions on the protected attribute.

There is a slight uptick in the true negative rate when we condition
on the unprivileged group (right more plot). The true positive rate
drops slightly here as well, with a rise in the false positive rate.
So the model is able to classify women with a lower income with high
accuracy. But the performance is 50-50 when it comes to women with a
higher income. And this again is corroborated by the fact that we
trained the model with very few examples of women with a high income.

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-lr_cm-gen'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['GTN', 'GFP', 'GFN', 'GTP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-lr_cm-gen.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-lr_cm-gen-rate'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['GTNR', 'GFPR', 'GFNR', 'GTPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-lr_cm-gen-rate.png]]

The =num_generalized_*= metrics use the probability associated with
the predicted label (rather than the absolute label). I assume there
is some sort of rounding up going on internally which results in the
true negative & true positive numbers to be exactly the same as the
data. It will be interesting to experiment here more & see when (and
if) these numbers change for variation in the dataset or model.

*** model: decisiontreeclassifier
Lets look at the decisiontree classifier next.

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-dt_cm'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['TN', 'FP', 'FN', 'TP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-dt_cm.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-dt_cm-rate'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['TNR', 'FPR', 'FNR', 'TPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-dt_cm-rate.png]]

The general trend is the same across both models: they are able to
detect the negative class well but fail to do so for the positive
class.

Compared to logisticregression, the decisiontreeclassifier performs
slightly worse. However, we must account for the fact that the model
is not tuned. The performance many increase with some effort invested
in model tuning.

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-dt_cm-gen'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['GTN', 'GFP', 'GFN', 'GTP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-dt_cm-gen.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-dt_cm-gen-rate'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['GTNR', 'GFPR', 'GFNR', 'GTPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-dt_cm-gen-rate.png]]

** Analysis of =base_rate=
The =base_rate= is the probability that the label of a given example
is positive.

#+begin_src python :results file
  name = 'adult_barplot_prot-sex_base-rate'

  fig, ax = plt.subplots()

  sns.barplot(data=data,
	      y='base_rate',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)

#+end_src

#+RESULTS:
[[file:adult_barplot_prot-sex_base-rate.png]]

Here, we note that the =base_rate= is similar across the subsets and
the conditions. This makes sense since we used random sampling to
generate the train & test subsets.

The unconditioned =base_rate= is ~25% and this makes sense since we
have more examples of the negative class. The conditioned =base_rate=
for the privileged group is higher than the unprivileged group (~30%
vs. ~11%). This makes sense as well since we have more examples of the
privileged group.

** Analysis of ={positive,negative}_predictive_value= & =false_{discovery,omission}_rate=
The wikipedia page on [[https://en.wikipedia.org/wiki/Binary_classification][binary classification]] was very helpful to make
sense of these metrics. Following is a table summarising their
mathematical formulas

| metric | formula    | alias              |
|--------+------------+--------------------|
| TPR    | TP/P       | recall/sensitivity |
| FPR    | FP/N       | 1 - TNR            |
| FNR    | FN/P       | 1 - TPR            |
| TNR    | TN/N       | specificity        |
| PPV    | TP/(TP+FP) | precision          |
| FDR    | FP/(TP+FP) | 1 - PPV            |
| FOR    | FN/(TN+FN) | 1 - NPV            |
| NPV    | TN/(TN+FN) |                    |

With the following model of confusion matrix (where =y_true= is on y
axis and =y_pred= is on x axis):

| y_true | 0 | TN     | FP     |
| y_true | 1 | FN     | TP     |
|        |   | 0      | 1      |
|        |   | y_pred | y_pred |

We visualise the above metrics in a confusion matrix like so:

| y_true | 0 | NPV    | FDR    |
| y_true | 1 | FOR    | PPV    |
|        |   | 0      | 1      |
|        |   | y_pred | y_pred |

*** model: logisticregression

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-lr_cm-ppv-fdr-for-npv'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['NPV', 'FDR', 'FOR', 'PPV']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-lr_cm-ppv-fdr-for-npv.png]]

+ [ ] review precision-recall trade-off; the rest of the metrics in
  these plots don't matter as much

*** model: decisiontreeclassifier

#+begin_src python :results file
  name = 'adult_heatmap_prot-sex_mod-dt_cm-ppv-fdr-for-npv'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['NPV', 'FDR', 'FOR', 'PPV']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-sex_mod-dt_cm-ppv-fdr-for-npv.png]]

** Analysis of =disparate_impact= & =statistical_parity_difference=
These metrics exist both for the data & the model so we should compare
them and see how they differ. For each metric, we create two plots:
First, we observe the distribution of the metric across the subsets.
And second we compare the distribution of the metric when calculated
with & without a model.

=disparate_impact= when calculated without a model, is expressed
mathematically as follows:

\begin{equation}
\frac{Pr(Y=1 | D = \text{unprivileged})}{Pr(Y=1 | D =
\text{privileged})}
\end{equation}

So intuitively, if we have more examples of positive class with the
privileged group (sex is 'Male'), the metric will approach 0. Ideally,
we want the metric to be high with a maximum value of 1 which
indicates that we have equal number of positive examples for both
privileged & unprivileged groups.

#+begin_src python :results file
  name = 'adult_barplot_prot-sex_mod-none_disparate-impact'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['model'] == 'None'],
	      y='disparate_impact',
	      x='subset',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-sex_mod-none_disparate-impact.png]]

The =disparate_impact= across the various subsets is low. And this
makes sense since we do not have that many examples of positive class
for the unprivileged group in our dataset.

#+begin_src python :results file
  name = 'adult_barplot_prot-sex_mod-all_disparate-impact'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='disparate_impact',
	      x='model',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-sex_mod-all_disparate-impact.png]]

When we calculate =disparate_impact= using a model, we use the
predictions instead of the actual label. The mathematical formula
changes to the following.

\begin{equation}
\frac{Pr(\hat{Y}=1 | D = \text{unprivileged})}{Pr(\hat{Y}=1 | D =
\text{privileged})}
\end{equation}

The =disparate_impact= in the model is similar to what we see in the
dataset. This makes sense since the model merely reflects the
statistics of the dataset.

- [ ] analyse the results across the models; I was under the
  impression that the decisiontreeclassifier was not performing well
  (based on the results in the confusion matrices), but that doesn't
  seem to be the case?

#+begin_src python :results file
  name = 'adult_barplot_prot-sex_mod-none_stat-par-diff'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['model'] == 'None'],
	      y='statistical_parity_difference',
	      x='subset',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-sex_mod-none_stat-par-diff.png]]

The =statistical_parity_difference= is expressed mathematically as
follows.

\begin{equation}
Pr(Y=1 | D = \text{unprivileged}) - Pr(Y=1 | D = \text{privileged})
\end{equation}

Intuitively, the value for this metric falls within the range of $[-1,
1]$. A value of 0 indicates that the dataset contains equal number of
positive examples for both privileged & unprivileged groups. A value
of -1 is not ideal since it indicates that the dataset contains
significantly more examples of the positive class with the privileged
group. The idea value for this metric thus lies between $[0, 1]$.

#+begin_src python :results file
  name = 'adult_barplot_prot-sex_mod-all_stat-par-diff'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='statistical_parity_difference',
	      x='model',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-sex_mod-all_stat-par-diff.png]]

Again, the metric is negative both in the data & model since we have
more examples of the positive class with the privileged group.

- [ ] analyse results across models

* Analysis of protected attribute =race=
In this section we expand & compare the metrics for both the protected
attributes.

#+begin_src python
  data = adult[adult['protected'] == 'race']
  data.shape
#+end_src

#+RESULTS:
| 15 | 32 |

** Analysis of =num_{positive,negative}=, =num_{true,false}_{positive,negative}= & =num_{true,false}_{positive_negative}_rate=

#+begin_src python :results file
  name = 'adult_barplot_prot-race_subset-all_num-pos-neg'

  fig, axs = plt.subplots(1, 2, sharey=True, figsize=(10, 5))

  sns.barplot(data=data,
	      y='num_positives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[0])

  for container in axs[0].containers:
      axs[0].bar_label(container)

  sns.barplot(data=data,
	      y='num_negatives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[1])

  for container in axs[1].containers:
      axs[1].bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-race_subset-all_num-pos-neg.png]]

#+begin_src python :results file
  name = 'adult_barplot_prot-race_subset-test_num-pos-neg'

  fig, axs = plt.subplots(1, 2, sharey=True, figsize=(10, 5))

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='num_positives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[0])

  for container in axs[0].containers:
      axs[0].bar_label(container)

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='num_negatives',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=axs[1])

  for container in axs[1].containers:
      axs[1].bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-race_subset-test_num-pos-neg.png]]

*** model: logisticregression

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-lr_cm'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['TN', 'FP', 'FN', 'TP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-lr_cm.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-lr_cm-rate'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['TNR', 'FPR', 'FNR', 'TPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-lr_cm-rate.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-lr_cm-gen'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['GTN', 'GFP', 'GFN', 'GTP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-lr_cm-gen.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-lr_cm-gen-rate'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['GTNR', 'GFPR', 'GFNR', 'GTPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-lr_cm-gen-rate.png]]

*** model: decisiontreeclassifier

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-dt_cm'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['TN', 'FP', 'FN', 'TP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-dt_cm.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-dt_cm-rate'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['TNR', 'FPR', 'FNR', 'TPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-dt_cm-rate.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-dt_cm-gen'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['GTN', 'GFP', 'GFN', 'GTP']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt="",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-dt_cm-gen.png]]

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-dt_cm-gen-rate'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['GTNR', 'GFPR', 'GFNR', 'GTPR']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-dt_cm-gen-rate.png]]

** Analysis of =base_rate=

#+begin_src python :results file
  name = 'adult_barplot_prot-race_base-rate'

  fig, ax = plt.subplots()

  sns.barplot(data=data,
	      y='base_rate',
	      x='subset',
	      hue='privileged',
	      hue_order=['None', 'True', 'False'],
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)

#+end_src

#+RESULTS:
[[file:adult_barplot_prot-race_base-rate.png]]

** Analysis of ={positive,negative}_predictive_value= & =false_{discovery,omission}_rate=

*** model: logisticregression

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-lr_cm-ppv-fdr-for-npv'
  metrics = data[data['model'] == 'logisticregression']
  cols = ['NPV', 'FDR', 'FOR', 'PPV']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-lr_cm-ppv-fdr-for-npv.png]]

*** model: decisiontreeclassifier

#+begin_src python :results file
  name = 'adult_heatmap_prot-race_mod-dt_cm-ppv-fdr-for-npv'
  metrics = data[data['model'] == 'decisiontreeclassifier']
  cols = ['NPV', 'FDR', 'FOR', 'PPV']
  fig, axs = plt.subplots(1, 3, figsize=(15, 5))

  for idx, privileged in enumerate(['None', 'True', 'False']):
      cm = metrics[metrics['privileged'] == privileged]
      cm = cm[cols].values.reshape(2,2)
      sns.heatmap(data=cm,
		  annot=cm,
		  fmt=".3f",
		  cbar=False,
		  cmap='Blues',
		  ax=axs[idx])
      axs[idx].set_xlabel("y_pred")
      axs[idx].set_ylabel("y_true")
      axs[idx].set_title(privileged)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_heatmap_prot-race_mod-dt_cm-ppv-fdr-for-npv.png]]

** Analysis of =disparate_impact= & =statistical_parity_difference=

#+begin_src python :results file
  name = 'adult_barplot_prot-race_mod-none_disparate-impact'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['model'] == 'None'],
	      y='disparate_impact',
	      x='subset',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-race_mod-none_disparate-impact.png]]

#+begin_src python :results file
  name = 'adult_barplot_prot-race_mod-all_disparate-impact'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='disparate_impact',
	      x='model',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-race_mod-all_disparate-impact.png]]

#+begin_src python :results file
  name = 'adult_barplot_prot-race_mod-none_stat-par-diff'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['model'] == 'None'],
	      y='statistical_parity_difference',
	      x='subset',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-race_mod-none_stat-par-diff.png]]

#+begin_src python :results file
  name = 'adult_barplot_prot-race_mod-all_stat-par-diff'

  fig, ax = plt.subplots()

  sns.barplot(data=data[data['subset'] == 'test'],
	      y='statistical_parity_difference',
	      x='model',
	      ax=ax)

  for container in ax.containers:
      ax.bar_label(container)

  utils.savefig(fig, name)
#+end_src

#+RESULTS:
[[file:adult_barplot_prot-race_mod-all_stat-par-diff.png]]

