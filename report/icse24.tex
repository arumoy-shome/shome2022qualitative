\documentclass[sigconf,review,anonymous]{acmart}

\usepackage{framed}
\newcommand{\highlight}[1]{\begin{framed}%
  \noindent\emph{#1}
\end{framed}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ICSE 2024]{46th International Conference on Software
  Engineering}{April 2024}{Lisbon, Portugal}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
%% \acmPrice{15.00}
%% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


\begin{document}

\title{Data vs. Model Machine Learning Fairness Testing: An Empirical Study}


\author{Arumoy Shome}
\affiliation{%
  \institution{Delft University of Technology}
  \city{Delft}
  \country{Netherlands}}
\email{a.shome@tudelft.nl}

\author{Lu{\'\i}s Cruz}
\affiliation{%
  \institution{Delft University of Technology}
  \city{Delft}
  \country{Netherlands}}
\email{l.cruz@tudelft.nl}

\author{Arie van Deursen}
\affiliation{%
 \institution{Delft University of Technology}
 \city{Delft}
 \country{Netherlands}
\email{arie.vandeursen@tudelft.nl}}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%% \renewcommand{\shortauthors}{Trovato et al.}

\begin{abstract}

  Preference has primarily been given to testing for robustness and
  correctness of ML systems while other non-functional properties such
  as fairness have been ignored. Although several fairness definitions
  and bias mitigation techniques exist in the literature, all existing
  solutions evaluate fairness after the training stage. This paper
  presents an empirical analysis of the relationship between model
  dependent and independent fairness metrics using $2$ fairness
  metrics, $4$ ML algorithms, $5$ real-world datasets and $1600$
  training and fairness evaluation cycles. We find a linear
  relationship between data and model fairness metrics when the
  distribution and the size of the training data changes. Our results
  indicate that testing for fairness prior to training can be a
  ``cheap'' and effective means of catching a biased data collection
  process early; detecting data drifts in production systems and
  minimising execution of full training cycles thus reducing
  development time and costs.

\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{SE4ML, ML Fairness Testing, Empirical Software
  Engineering, Data-centric AI}
\maketitle

\section{Introduction}\label{sec:intro}

While several contributions toward testing ML systems have been made
in recent years, preference has primarily been given to robustness and
correctness while other non-functional properties such as security,
privacy, efficiency, interpretability and fairness have been
ignored \cite{zhang2020machine,zhang2021ignorance,mehrabi2021survey,wan2021modeling}. Testing
for fairness in ML systems however, is a multi-faceted problem and
involves both technological and social factors. Although an abundance
of definitions for fairness and consequently techniques to mitigate
said bias exists in the scientific literature, all existing solutions
evaluate fairness after the training stage, using the predictions of
the ML model.

In addition to the underlying codebase of ML software systems, both
the training data and the ML algorithms constantly evolve and change
throughout the ML development
lifecycle \cite{sculley2015hidden,bosch2021engineering,hutchinson2021towards}.
ML systems comprise of several stages that are tangled with one
another. Testing such systems is challenging since a single change in
any of the stages can initiate a ``chain-reaction'' throughout the
entire system. Testing ML systems is also expensive since it involves
a full training-testing cycle which incurs financial costs as the size
of the data and the complexity of the model increases. Thus, catching
fairness bugs in the early, upstream stages of the ML lifecycle
becomes extremely valuable \cite{shome2022data}.

In contrast to prior work, we take a more holistic approach by testing
for fairness at two distinct locations of the ML development
lifecycle. First, prior to model training using fairness metrics that
can quantify the bias in the training data (henceforth Data Fairness
Metric or DFM). And second, after model training using fairness
metrics that quantify the bias in the predictions of the trained model
(henceforth Model Fairness Metric or MFM).

While MFM has been widely adopted in practise and well researched in
academia, we do not yet know the role of DFM when testing for fairness
in ML systems. The research goal of this study is to evaluate the
effectiveness of DFM for catching fairness bugs. We do this by
analysing the relationship between DFM and MFM through an extensive
empirical study. The analysis is conducted using $2$ fairness metrics,
$4$ ML algorithms, $5$ real-world tabular datasets and $1600$ fairness
evaluation cycles. To the best of our knowledge, this is the first
study which attempts to bridge this gap in scientific knowledge. Our
results are exploratory and open several intriguing avenues of
research.

The research questions along with the contributions of this paper are
as follows.
\begin{description}
  \item[RQ1.] What is the relationship between DFM and MFM as the
    fairness properties of the underlying training dataset changes?

    DFM and MFM convey the same information when the distribution of
    the underlying training dataset changes. This implies that DFM can
    be used as early warning systems to catch data drifts in
    production ML systems that may affect its fairness.

  \item[RQ2.] How does the training sample size affect the
    relationship between DFM and MFM?

    Our analysis of the training sample size and how it influences the
    relationship between DFM and MFM reveals the presence of a
    trade-off between fairness, efficiency and correctness. In
    Section \ref{sec:discuss-fair-eff-perf-trade} we provide some
    practical guidelines on how to best navigate this trade-off.

  \item[RQ3.] What is the relationship between DFM and MFM across
    various training and feature sample sizes?

    DFM and MFM convey the same information when the training sample
    size changes. This implies that DFM can help practitioners catch
    fairness issues upstream and avoid execution costs of a full
    training cycle.
\end{description}

All source code and results of the study are publicly accessible under
the CC-BY 4.0
license\footnote{https://figshare.com/s/67206f7c219b12885a6f}.

The remainder of the paper is structured as follows.
Section \ref{sec:related} summarises related concepts and prior work
done in the field of ML fairness testing. In Section \ref{sec:method},
the experimental design and fairness evaluation strategy is presented.
The results of this study are presented in Section \ref{sec:results}
and their implications are discussed in
Section \ref{sec:implications}. The paper concludes with a few notes
on the threats to validity in Section \ref{sec:threats} and future
work in Section \ref{sec:conclude}.

\section{Preliminaries}\label{sec:related}

This section provides a summary of the relevant concepts and prior
work done in ML fairness testing. The section concludes with an
overview of the existing ML development lifecycle and positions the
proposed used of DFM within the stages of the lifecycle in contrast to
MFM.

\subsection{Algorithmic Bias, Bias Mitigation and Group Fairness}\label{sec:bias-fairness}

Manually validating the fairness of the labels is often an expensive
and time consuming process which is still prone to cognitive and
social biases of the human auditors. Significant efforts have
therefore been made to quantify the bias present in a ML model using
fairness metrics. Existing fairness metrics are restricted to
supervised binary classification problems where one of the outcomes is
more favourable than the other and the dataset contains one or more
\emph{protected attributes} such as \emph{race, sex, age, colour,
religion or disability status}. An ML model is said to make unfair
decisions if it favours a certain group or individual pertaining to
one or more protected attributes in the dataset.

Fairness metrics can be broadly classified into two
categories---namely, \emph{group fairness} and \emph{individual
fairness}. Individual fairness dictates that the predictions of an ML
model should not differ for two individuals who only differ in the
value of the protected attribute. While group fairness dictates that
the predictions of an ML model should be similar for both privileged
and unprivileged groups present in the
dataset \cite{castelnovo2022clarification,hellman2020measuring,mitchell2021algorithmic,kusner2017counterfactual,grgic2016case,dwork2012fairness,barocas2019fairness,barocas2016big,hardt2016equality,binns2018fairness,verma2018fairness,saxena2019fairness}.

Once an appropriate definition of fairness is identified, the relevant
techniques to mitigate said bias must be found. Bias mitigation
techniques can be classified into three groups based on the location
where the mitigation technique is applied: \emph{pre-processing,
in-processing and post-processing}. Several in-processing techniques
or novel ML algorithms have been proposed that take fairness into
consideration while training from biased
data \cite{zhang2018mitigating,agarwal2018reductions,kearns2018preventing,kamishima2012fairness}.
In contrast to in-processing techniques which target the ML model,
pre-processing
 \cite{feldman2015certifying,zemel2013learning,calmon2017optimized,kamiran2012data}
and post-processing
 \cite{pleiss2017fairness,hardt2016equality,kamiran2012decision}
techniques are applied to the training data and the predictions of the
ML model respectively.

This study uses group fairness metrics due to their popularity in
existing empirical studies on ML fairness testing and ease of
understandability \cite{zhang2021ignorance,biswas2020machine,biswas2021fair,hort2021fairea,chakraborty2021bias}.
Our analysis of the relationship between the DFM and MFM (see
Section \ref{sec:implications}) presents practical guidelines for
practitioners to pick the appropriate bias mitigation strategy based
on the particular fairness issue they are facing.

\subsection{Prior Work in ML Fairness Testing}\label{sec:prior-work}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{ml-lifecycle.pdf}
  \caption{Stages of the ML Lifecycle (adopted from
    \cite{amershi2019software,haakman2020ai,breck2019data}). Three
    distinct phases of the lifecycle are marked by different colours.
    Stages in the experimental and production phases may loop back to
    any prior stages, indicated by the large grey arrows. The location
    of fairness testing using DFM and MFM are marked by the green
    labels. The green arrow depicts the shift proposed by this study
    in ML fairness testing. }
  \label{fig:ml-lifecycle}
\end{figure*}

There is a growing consensus amongst academics that not all fairness
metrics can be satisfied simultaneously. There is also a consensus
that fairness and performance of ML systems are orthogonal to one
another and involve a trade-off. In fact, identifying the correct
fairness metric is the primary challenge which typically depends on
the domain and problem at hand. It is therefore recommended to
consider fairness as early as the requirements engineering and
software design
phase \cite{zhang2020machine,chen2022fairness,mehrabi2021survey,zhang2021ignorance}.

Several literature surveys have been conducted to classify and
catalogue various ML fairness testing and bias mitigation techniques.
\citeauthor{wan2021modeling} conducted a large scale survey on
in-processing bias mitigation strategies and their effectiveness.
\citeauthor{chen2022fairness} and \citeauthor{mehrabi2021survey}
conducted a survey of existing literature on fairness testing in ML
systems. \citeauthor{mehrabi2021survey} present a comprehensive survey
on the state-of-the-art research on fairness in ML with an emphasise
on fairness issues arising from both the data and the model.
\citeauthor{chen2022fairness} survey 113 papers addressing fairness
testing and provide a formal definition of fairness bugs and fairness
testing in ML from a software engineering perspective. Authors also reflect on how
fairness testing differs from traditional software testing and provide
practical guidelines on how and where to test for fairness within the
entire Software Development
Lifecycle \cite{wan2021modeling,chen2022fairness,mehrabi2021survey}.

Prior work has also focused on conducting empirical analysis of bias
mitigation techniques. \citeauthor{biswas2021fair} take a holistic
view of the entire ML pipeline and analyse the effect of common data
pre-processing techniques such as standardisation, feature selection,
encoding and sampling on the fairness of ML models. The analysis is
conducted using 37 real-world ML pipelines from Kaggle notebooks which
operate on five datasets. Typically data for the unprivileged group
tends to be limited which make pre and post processing bias mitigation
techniques less effective. \citeauthor{feffer2022empirical} thus
conduct an empirical analysis of bias mitigation in combination with
popular ensemble techniques to understand the effectiveness of such
combinations. \citeauthor{zhang2021ignorance} studied the effect of
training sample and feature sample size on the fairness of ML
models. Authors observe that a large feature sample combined with
a small training set helps reduce
bias \cite{biswas2021fair,feffer2022empirical,zhang2021ignorance}.

Prior work discussed so far operate under the assumption that the
training data and learning algorithm are accessible to the ML
practitioner---often referred to as \emph{white-box testing}. In
contrast, \emph{black-box testing} makes no such assumptions and
treats the entire ML pipeline as a black-box where we can only control
the input to the system and see the corresponding output. For such
situations, several test input generation techniques have been
proposed. \citeauthor{galhotra2017fairness} proposed \emph{Themis},
a tool which automatically generates a testing suite to measure
discrimination in software using causal fairness
testing. \citeauthor{udeshi2018automated} propose \emph{Aequitas}, an
automated tool that accepts a model and the protected attributes as
input and explores the input space to detect specific examples that
may produce discriminatory behaviour in the
model. \citeauthor{aggarwal2019black} propose a new technique for
generating test input using symbolic execution which accounts for
correlation amongst the protected and unprotected
attributes \cite{aggarwal2019black,udeshi2018automated,galhotra2017fairness}.

This study conducts an empirical analysis of the relationship between
DFM and MFM, and as such, operates under white-box testing
assumptions. The experimental design of this study is similar in
spirit to that proposed by \citeauthor{zhang2021ignorance}. However
our objective, results and implications are entirely different. While
\citeauthor{zhang2021ignorance} study the effect of training and
feature sample size on the fairness of the model, this study aims to
understand the relationship between DFM and MFM. We analyse how change
in the distribution, sample size and number of features in the
training set affects this relationship. Through this analysis, we hope
to establish the role of DFM within the existing ML development
lifecycle.

\subsection{Machine Learning Lifecycle and Fairness
  Testing}\label{sec:ml-lifecycle}

Figure \ref{fig:ml-lifecycle} presents an overview of the various
stages of the ML lifecycle adapted from prior scientific literature.
The stages can be classified into three distinct phases as depicted by
the different colours. Although the figure presents a linear flow
amongst the various phases of the lifecycle, there are often several
feedback loops that may influence one another. The experimental and
production phases are marked with a gray arrow in the top right corner
as any stage in these phases can loop back to any prior stage in the
lifecycle \cite{amershi2019software,haakman2020ai,breck2019data}.

The upstream stages in the start-up phase tend to be ``data-centric''.
ML projects within the technology section typically work with
pre-existing data. However, in the recent shift towards application of
ML to safety critical domains, the ground truth must be established
through data collection, cleaning and
labelling \cite{sambasivan2021everyone,bosch2021engineering,amershi2019software}.

With the data ready, a highly experimental phase begins. This phase
involves several iterations to identify and engineer important data
features that boost the performance of the ML model. Historically, the
performance of ML models has been based on functional properties such
as the correctness and model relevance. However, in recent years,
non-functional properties such as robustness, fairness, security and
explainability have also been included \cite{zhang2020machine}.

The finalised model is deployed in a production environment to make
predictions on unseen data. ML systems---unlike traditional software
systems---do not fail explicitly in case of an error. But continue to
operate, albeit with a degraded performance. Since the output of the
ML model may be utilised by other software systems, the health of
production ML systems are continually monitored. Data will constantly
evolve and change as a reflection of the real world. However, ML
models are trained and tested using historical data which become
``stale'' after a certain period of time. Once the performance of the
live ML model drops below a pre-defined threshold, a new
training-testing-deployment cycle is issued. The next batch of
training data is typically derived by combining the unlabelled data in
production with the predictions of the live
model \cite{breck2019data,hynes2017data,breck2017ml}.

The status quo in ML fairness testing is to test for fairness after
the model has been trained, using the predictions of the model. In
this study, we wish to understand the benefits and limitations of
evaluating fairness prior to model training. This proposed shift in ML
fairness testing is marked by the green arrow in
Figure \ref{fig:ml-lifecycle}.

\section{Experimental Design}\label{sec:method}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{method.pdf}
  \caption{Methodology for evaluating fairness of datasets and ML
  models using DFM and MFM.}
  \label{fig:method}
\end{figure*}

This section presents details regarding the experimental design of
this study. The datasets, ML models and fairness metrics used in this
study are mentioned and the methodology used to evaluate DFM and MFM
is presented (summarised in Figure \ref{fig:method}). The section
concludes with an analysis of the experimental data collected in this
study.

\subsection{Datasets, ML Models and Fairness Metrics}\label{sec:method-parameters}

\begin{table}
  \centering
  \caption{Fairness metrics used in this study}
  \begin{tabular}{l r}
    \toprule
    \textbf{\emph{DFM}}\\
    \midrule
    DI & \(\displaystyle \frac{P(Y=1|D=0)}{P(Y=1|D=1)}\)\\
    SPD & \(\displaystyle P(Y=1|D=0)-P(Y=1|D=1)\)\\
    \midrule
    \textbf{\emph{MFM}}\\
    \midrule
    DI & \(\displaystyle \frac{P(\hat{Y}=1|D=0)}{P(\hat{Y}=1|D=1)}\)\\
    SPD & \(\displaystyle P(\hat{Y}=1|D=0)-P(\hat{Y}=1|D=1)\)\\
    \bottomrule
  \end{tabular}
  \label{tab:fairness-metrics}
\end{table}

Table \ref{tab:fairness-metrics} shows the group fairness metrics
along with their mathematical formulas used in this study. We include
all group fairness metrics---namely \emph{Disparate Impact (DI)} and
\emph{Statistical Parity Difference (SPD)}---for which both model
dependent and independent variants are available. The DFM use the
labels of the data ($Y$) where as the MFM use the predictions of the
trained ML models ($\hat{Y}$). Favourable and unfavourable outcomes
are represented by $1$ and $0$ respectively. Similarly, privileged and
unprivileged groups of the protected attribute ($D$) are represented
by $1$ and $0$ respectively. All fairness metrics and datasets used in
this study are obtained from the \emph{AIF360} python
library \cite{bellamy2019ai}.

\begin{table}
  \centering
  \caption{Datasets used in the study}
  \begin{tabular}{l l r}
    \toprule
    \textbf{Name} & \textbf{Prot.} & \textbf{\#Eg.}\\
    \midrule
    German \cite{hofmann1994german} & age, sex & 1000\\
    Compas\cite{angwin2016machine} & race, sex & 6167\\
    MEPS \cite{mepsdata} & race & 15675\\
    Bank\cite{moro2014data} & age & 30488\\
    Adult\cite{kohavi1996scaling} & race, sex & 45222\\
    \bottomrule
  \end{tabular}
  \label{tab:datasets}
\end{table}

Table \ref{tab:datasets} presents the datasets used in this study. We
consider tabular datasets which have been extensively used in prior
scientific contributions on ML fairness
testing \cite{zhang2021ignorance,biswas2020machine,biswas2021fair,chen2022fairness}. Based
on prior work, we only consider one protected attribute at any given
time thus giving us eight independent datasets. We follow the default
pre-processing steps implemented in the AIF360 library---missing
values are dropped and categorical features are label encoded. Prior
to training, the features in the training and testing subsets are
standardised by removing the mean of the sample and scaling to unit
variance.

We use the scikit-learn \cite{pedregosa2011scikit} python library for
creating the train-test splits and training the ML models. We use four
ML models of varying complexity namely, \emph{Logistic Regression},
\emph{Decision Trees}, \emph{Random Forest} and \emph{Ada boost} based
on their popularity in practise and in prior scientific
publications \cite{zhang2021ignorance,biswas2021fair,biswas2020machine}.

\subsection{Fairness Evaluation}\label{sec:method-fair-eval}

\begin{table}
  \centering
  \caption{Parameters of the study}
  \begin{tabular}{l r}
    \toprule
    \textbf{Parameter} & \textbf{Count}\\
    \midrule
    Fairness metrics & $2$\\
    ML models & $4$\\
    Datasets & $8$\\
    Total cases & $8\times4=32$\\
    Iterations & $50$\\
    Total fairness evaluation cycles & $32\times50=1600$\\
    \bottomrule
  \end{tabular}
  \label{tab:parameters}
\end{table}

Figure \ref{fig:method} presents the methodology used in this study
for evaluating the fairness of ML models and datasets. A 75--25 split
with shuffling is used to create the training and testing splits. DFMs
and MFMs are used to quantify the bias in the underlying distribution
of the training set and the predictions of the models respectively. We
adopted the transformation steps from prior work to scale all fairness
metric values between $0$ and $1$ such that higher values indicate
more bias \cite{zhang2021ignorance,hort2021fairea}.

We extend the above experiment further in two ways. First, we
experiment with different number of examples and second with different
number of features in the training set. For both experiments, we
shuffle the order of the examples in the training and testing sets.
Additionally, for the feature sample size experiment we shuffle the
order of the features.

For the training sample size experiment, we generate different
training samples of varying sizes starting from 10\% of the original
training data, and increase in steps of 10\% until the full quantity
is reached. For the feature sample size experiment, we start with
a minimum of three features (in addition to the protected attribute
and target) and introduce one new feature until all the features are
utilised. Both the training and testing sets undergo the same feature
sampling procedure in the feature sample size experiment. No such
sampling is done in the testing set for the training sample size
experiment.

Table \ref{tab:parameters} summarises the parameters of the study. We
train 4 ML models on 8 datasets producing 32 total cases. The fairness
for each case is evaluated 50 times using two fairness metrics, thus
producing a total of 1600 training and fairness evaluation cycles.

\subsection{Correlation Analysis}\label{sec:corr-analysis}

We use correlation analysis to study the relationship between DFM and
MFM with-respect-to change in three experimental
factors---distribution, size and features of the training set.
Spearman Rank Correlation is used to quantify the linear relationship
between the DFM and MFM since it does not assume normality and is
robust to outliers. We repeat all experiments 50 times and report the
statistical significance of our results. We consider cases where
$pvalue\le0.05$ to be statistically significant in our evaluation.

A positive correlation means that the DFM and MFM changed in the same
direction and thus \emph{convey the same information}. A positive
correlation indicates that both the DFM and MFM show presence of bias
in the training data and in the model predictions respectively. A
negative correlation between the DFM and MFM means that they changed
in the opposite direction and \emph{do not convey the same
information}. This indicates that the bias in the model predictions
was lower than that present in the training data.

%% TODO should we explain why negative correlation from the inverse is
%% not possible (ie. DFM reports lower than MFM)? this makes things a
%% bit more complicated and we do not have sufficient evidence in our
%% results to back this up.

The implications of the correlation results varies based on the
experimental factor. A detailed discussion is presented in the
corresponding sub-sections of Section \ref{sec:implications}.

\subsection{Distribution of Experimental Data}\label{sec:data-analysis}

%% TODO unsure how to indicate that we have data from full training
%% set and 50 iterations
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{boxplot--dataset--di-spd--exp-full.pdf}
  \caption{Boxplot showing distribution of DFM and MFM for all
    datasets, models and fairness metrics.}
  \label{fig:boxplot--dataset--di-spd--exp-full}
\end{figure}

Figure \ref{fig:boxplot--dataset--di-spd--exp-full} presents a boxplot
with the distribution of DFM and MFM for all datasets, models and
fairness metrics. The x-axis represents the datasets used in this
study while the y-axis presents the value of the fairness metric. The
models used in this study are represented using different
colours---note that the model ``None'' represented in blue refers to
the DFM. Both the fairness metrics DI (top) and SPD (bottom) are
presented in separate plots.

Comparing the boxplots for DI and SDP, we note that they follow a
similar pattern of distribution. Comparing the DFM and MFM for any
given ML model and dataset, we observe a difference in the
distribution of the DFM and MFM. In several instances the tree-based
classifiers (DT and RF) make fairer decisions compared to the other
classifiers, sometimes even better than the baseline provided by the
DFM.

The variability of MFM tends to be higher than DFM. This is because,
in addition to the randomness from the data shuffling in the training
set, the models are assigned random initial states in every iteration
thus resulting in different predictions across the iterations and
consequently varying MFM values.

\section{Results}\label{sec:results}

%% TODO here we should reiterate that we use correlation; how we
%% calculate it; and that its on the 50 iterations; refer back to the
%% relevant section

This section presents the analysis of the relationship between DFM and
MFM. The section is broken further into three subsections based on the
research questions.

\subsection{RQ1. What is the relationship between DFM and MFM as
the fairness properties of the underlying training dataset
changes?}\label{sec:results-full-rel}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{shuffle.pdf}
  \caption{Visual explanation of rationale for using smaller training
    sample to simulate change in the distribution of the training
    data. The grey boxes represent the full dataset while the blue
    boxes represent the training set for three hypothetical
    iterations. More overlap in the blue boxes depicts less
    distribution change and vice-versa.}
  \label{fig:shuffle}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{training-set-frac-threshold.pdf}
  \caption{\emph{\textbf{(left)}} Lineplot showing relationship
    between performance metrics and training sample size in the
    \emph{german-age} dataset. Data from the 50 iterations is
    aggregated using the mean, the error bars show the standard
    deviation. \emph{\textbf{(right)}} Countplot showing number of
    cases with significant change in accuracy and f1 when trained
    using the full vs. smaller training sample size.}
  \label{fig:training-set-frac-threshold}
\end{figure}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{heatmap--corr--training-sets-frac.pdf}
  \caption{Heatmap showing correlation between DFM and MFM for all
    models and datasets using training data with simulated
    distribution change. Each block is representative of 50
    iterations. The statistically significant cases are marked with an
    asterisks. We primarily observe bright hues of red indicating that
    the DFM and MFM convey the same information. This means that DFM
    can be used to identify fairness related data drifts in automated
    ML pipelines.}
  \label{fig:heatmap--corr--training-sets-frac}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{heatmap--corr--full-data.pdf}
  \caption{Heatmap showing correlation between DFM and MFM using
    training data without significant distribution change. Each block
    is representative of 50 iterations. In contrast to
    Figure \ref{fig:heatmap--corr--training-sets-frac}, we primarily
    observe darker colours indicating that the DFM and MFM no longer
    convey the same information.}
  \label{fig:heatmap--corr--full-data}
\end{figure*}

To simulate change in the distribution of the training data, we use a
smaller subset of the original training data. Figure \ref{fig:shuffle}
visually motivates this approach. Privileged examples ($D=1$) are
represented using squares while unprivileged examples ($D=0$) are
represented using circular points. The label of the examples are
represented using colour---green for favourable ($Y=1$) and red for
unfavourable ($Y=0$) examples. The large grey boxes represent the
original dataset while the blue boxes represent the training dataset.

The figures present three hypothetical iterations of the data sampling
process to create the training set. The left figure presents the
scenario where the full training set is used while the right figure
presents the scenario where a smaller portion of the full training set
is used. As seen from the figure, there is more overlap amongst the
examples in the full training set in every iteration. In contrast,
there is less overlap amongst the examples in the smaller training
set. The distribution in smaller training samples will thus change
more frequently in the 50 iterations and capture a wide variety of
data fairness properties.

The data quality in smaller training samples will however deteriorate.
To identify a sample size that captures a variety of data fairness
properties while also being a realistic training dataset, we analyse
the \emph{accuracy} and \emph{f1 score} of the models across different
training sample sizes (as explained in
Section \ref{sec:method-fair-eval}). Next, we conduct \emph{student
t-test} to identify the smallest sample size where the performance of
the models is similar to that obtained when trained using the full
training set.

Figure \ref{fig:training-set-frac-threshold} (right) presents a
histogram of the number of cases where there was a significant
difference between the two populations. We note that there is a
significant difference in the performance of the models, in the
majority of the cases when the training size is reduced to 50\%. The
performance however remains consistent when using a training size of
60\% or higher. This is also corroborated by the lineplot in
Figure \ref{fig:training-set-frac-threshold} (left) which shows the
accuracy and f1 of all models across various training sample sizes in
the \emph{german-age} dataset. We observe that the performance
stabilises starting from 60\% training sample size. Thus for the
majority of the cases, a training sample of 60\% allows us to train
models with acceptable performance, while also capturing a wide
variety of fairness issues in the underlying training data within the
50 iterations.

Figure \ref{fig:heatmap--corr--training-sets-frac} shows the
correlation between the DFM and MFM across all models and datasets
when trained using data with simulated distribution change as outlined
above. The models used in this study are represented along the y-axis
and the datasets along the x-axis. Darker colours indicate weaker
correlation whereas brighter colours indicate stronger correlation.
Positive correlation is indicated using hues of red while negative
correlation is indicated using hues of blue. The correlation between
DFM and MFM for both fairness metrics are shown separately. We
primarily observe a positive correlation between the DFM and MFM. This
indicates that the DFM and the MFM convey the same information as the
distribution---and consequently the fairness properties---of the
underlying training dataset changes.

In contrast to Figure \ref{fig:heatmap--corr--training-sets-frac},
Figure \ref{fig:heatmap--corr--full-data} shows the correlation
between DFM and MFM when trained using data without sufficient
distribution change. Due to lack of significant change in the
distribution of the training data, we primarily observe darker colours
indicating that the DFM and MFM are not linearly related to one
another anymore.

\highlight{\textbf{Answer to RQ1:} DFM and MFM are positively
correlated and thus convey the same information as the
distribution---and consequently the fairness properties---of the
underlying training dataset changes.}

\subsection{RQ2. How does the training sample size affect the
correlation between DFM and MFM?}\label{sec:results-corr-frac}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{lineplot--frac--corr.pdf}
  \caption{Lineplot showing relationship of correlation (between DFM
    and MFM) with training sample size for all datasets and models.
    Data from the 50 iterations is aggregated using the mean value.
    The correlation decreases as the training sample size is
    increased. This indicates that with sufficient training data, ML
    models are able to circumvent some of the bias in the underlying
    training set.}
  \label{fig:lineplot--frac--corr}
\end{figure}

In Figure \ref{fig:heatmap--corr--training-sets-frac}, the correlation
in the smaller datasets are more positive compared to the larger
datasets when trained using a smaller training sample size. When the
training sample size is increased, the correlation in the datasets
decrease as observed in Figure \ref{fig:heatmap--corr--full-data}.

Based on these observations, we hypothesise that the quantity of
training data influences the correlation between DFM and MFM. Our
hypothesis is corroborated by Figure \ref{fig:lineplot--frac--corr}
which shows the relationship of the correlation with various training
sample sizes, for all datasets and models. The x-axis presents the
training sample size and the y-axis presents the correlation between
the DFM and MFM. The colours represent the models while the style of
the line represents the two fairness metrics. Each dataset is shown as
a separate subplot. The overwhelming majority show that the
correlation between the DFM and MFM decreases as we increase the
training sample size.

\highlight{\textbf{Answer to RQ2:} The training sample size has
a profound effect on the relationship between the DFM and MFM. The
correlation between the DFM and MFM decreases as we increase the
training sample size.}

\subsection{RQ3. What is the relationship between DFM and MFM
across various training and feature sample sizes?}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{heatmap--corr--frac.pdf}
  \caption{Heatmap showing correlation between DFM and MFM across all
    training sample sizes. Each block is representative of 50
    iterations $\times$ 10 training sample size $=$ 500 data points.
    The statistically significant cases are marked with an asterisks.
    We primarily observe hues of red indicating that the DFM and MFM
    convey the same information. When experimenting with the training
    sample size, DFM can aid practitioners catch fairness issues
    upstream and avoid execution costs of a full training cycle.}
  \label{fig:heatmap--corr--frac}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{heatmap--corr--num-features.pdf}
  \caption{Heatmap showing correlation between DFM and MFM across
    various feature sample sizes. Each block is representative of data
    points from 50 iterations $\times$ the number of features in the
    dataset. We primarily observe darker colours indicating that the
    DFM and MFM do not convey the same information. When experimenting
    with the feature sample size, practitioners must test for fairness
    both before and after training.}

  \label{fig:heatmap--corr--num-features}
\end{figure*}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{lineplot--num-features--corr.pdf}
  \caption{Lineplot showing relationship of correlation (between DFM
    and MFM) with feature sample size for all datasets and models.
    Data from the 50 iterations is aggregated using the mean value.
    There is no discernible relationship since the number of features
    does not influence the DFM.}
  \label{fig:lineplot--num-features--corr}
\end{figure}

\subsubsection{Training Sample Size}

In this section we analyse the relationship between DFM and MFM across
varying training sample sizes. In contrast to
Section \ref{sec:results-corr-frac} where we calculated the
correlation between DFM and MFM within each training sample size, here
we calculate the correlation across all training sample sizes. The
correlation between the DFM and MFM is shown in
Figure \ref{fig:heatmap--corr--frac}. We primarily observe colours
indicating that the DFM and MFM convey similar information as the
training sample size changes.

\subsubsection{Feature Sample Size}

In this section we analyse the relationship between the DFM and MFM
across varying feature sample sizes. In contrast to the training
sample size experiment above, we change the number of features in the
training set and randomise the feature order in each iteration.
Figure \ref{fig:heatmap--corr--num-features} presents the correlation
between the DFM and MFM across all feature sample sizes. We primarily
notice darker colours indicating that there is no significant
correlation between the DFM and MFM as the number of features in the
training dataset changes.

From Table \ref{tab:fairness-metrics}, we note that the feature sample
size does not affect the DFM thus explaining the lack of significant
correlation between the DFM and MFM. The larger datasets show a more
negative correlation. As explained in
Section \ref{sec:results-corr-frac}, this is because the larger
datasets have sufficient training data for the model to mitigate some
of the bias in the underlying training set. This can also be verified
in Figure \ref{fig:lineplot--num-features--corr} which shows the
relationship between the correlation and the feature sample sizes for
all datasets and models. There is no discernible relationship between
the correlation and the feature sample size in the top row containing
datasets which are smaller in size but contain a large number of
features. A slight relationship can only be observed in the bottom
right subplots which contain datasets which are larger in size but
contain less number of features.

\highlight{\textbf{Answer to RQ3:} DFM and MFM convey similar
information as the training sample size changes but not when the
feature sample size changes.}

\section{Implications}\label{sec:implications}
The implications of the results presented in Section \ref{sec:results}
are discussed in this section.

\subsection{Data Drift}\label{sec:discuss-data-drift}

Results from RQ1 indicate that the DFM and MFM convey the same
information when the distribution and consequently the fairness
properties of the training data changes. ML systems running in a
production environment are often monitored to detect degradation in
model performance. As shown in Figure \ref{fig:ml-lifecycle}, a
standard practise is to combine the data encountered by the model in
the production environment with its predictions to create the training
set for the next iteration \cite{biessmann2021automated}. Since data
reflects the real world, change in its underlying distribution over
time is eminent. Our results indicate that DFM can be used as a early
warning system to identify fairness related data drifts in automated
ML pipelines.

\subsection{Fairness, data size and correctness trade-off}\label{sec:discuss-fair-eff-perf-trade}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{tradeoff.pdf}
  \caption{Visual representation of trade-offs between fairness, data
    size and correctness of ML models.}
  \label{fig:tradeoff}
\end{figure}

%% TODO jump between positive correlation and lack of sufficient data
%% is not clear...
Results from RQ2 indicate that the quantity of training data
profoundly influences the relationship between DFM and MFM.
Figure \ref{fig:tradeoff} presents a visual explanation of the RQ2
results along with its implications to fairness and performance (or
correctness) of ML models. The top subplot visually summarises the RQ2
results observed in Figure \ref{fig:lineplot--frac--corr}. We
primarily see a positive correlation between the DFM and MFM in
smaller training sample sizes which indicates presence of bias in both
training data and model predictions. Thus a positive correlation
between the DFM and MFM may indicate lack of sufficient training data.
Under such circumstances, practitioners can either choose to collect
more data if possible or use bias mitigation techniques to address the
fairness issue.

% do we have proof of this claim?
The middle subplot depicts the effect of training sample size on DFM
and MFM. As the training data is increased, DFM remains consistent
while MFM reduces. Thus, with sufficient training data, the
correlation starts to drop and eventually becomes negative. This
indicates that the models learn to make fairer predictions and are
able to circumvent the bias in the training data to a certain extent.
A lower MFM value compared to DFM does not however guarantee the
absence of bias in the model. \citeauthor{zhang2021ignorance} showed
that in addition to the quantity of training data, the quality itself
affects the fairness of ML systems. Introducing more data does not fix
the bias in the model if the final distribution remains biased.

The bottom subplot depicts the effect of training sample size on the
correctness of the ML models. Depending on the complexity of the ML
model, its performance stabilises beyond a certain quantity of
training data. However, this training sample size may be smaller than
what is required to let the ML model repair some of the bias in the
dataset automatically. This presents a trade-off between efficiency
and performance. A slight reduction in the training sample size---in
combination with bias mitigation techniques and a rich feature sample
size---can allow practitioners to build fair ML systems with quicker
training cycles at the cost of negligible predictive
performance \cite{verdecchia2022data,zhang2021ignorance}. Engineering
efficient, high quality training data can reduce training cycles,
development time and ultimately project costs. Compounded over the
entire duration that an ML system stays in production along with the
human effort required to keep such a system operational, the benefits
can be more than substantial.

No correlation between DFM and MFM presents a trade-off between
fairness, efficiency and performance. Practitioners may opt for a more
efficient system by reducing the training sample size. However this
may reduce the predictive performance of the model and require more
engineering effort to mitigate the fairness issues in the system.
Alternatively, they may opt for more accurate predictions by using a
larger training sample size. A larger training size may mitigate some
of the bias in the training set at the cost of more compute.

\subsection{Test Reduction}\label{sec:discuss-test-red}

Results from RQ3 indicate that the DFM and MFM are positively
correlated and thus convey the same information when the training
sample size changes. DFM can therefore aid practitioners catch
fairness issues upstream and avoid execution costs of a full training
cycle. Considering the entire duration that an ML system is
operational, along with the multiple iterations it takes to test an ML
system, avoiding a full training cycle while evaluating its fairness
can be energy efficient and sustainable in the long run.

However the same test reduction cannot be made when experimenting with
the feature sample size of the training
set. \citet{zhang2021ignorance} showed that a larger feature sample
size typically improves the fairness of the model. To the best of our
knowledge, there are no fairness metrics that consider the influence
of other features on the fairness at the data level. Thus when
experimenting with the feature sample size, it is recommended that ML
practitioners evaluate the fairness both before and after training.

\subsection{Locating root cause of bias}\label{sec:discuss-root-cause-bias}

% TODO we have to mention chakraborty2021bias here; they do root cause
% of bias in ML!
Testing for fairness after training the model makes it very difficult
to identify where exactly the bias was introduced. Testing for
fairness both at the data (using DFM) and model (using MFM) level
provides a holistic view on the fairness of the entire system and can
aid practitioners identify the root cause of bias. If the DFM
indicates presence of bias, it can be an early indication of flaws in
the data collection process or flaws in the initial design of the
system itself. In the event that the DFM does not indicate bias but
the MFM does, practitioners can narrow down the cause of bias to the
learning algorithm itself and opt for in-processing or post-processing
bias mitigation techniques.

\subsection{Explaining fairness in decision trees}\label{sec:discuss-explain-fair-dt}

We consistently observe that Decision Trees (DTs) are able to make
fairer predictions with minimal effort. For instance, in
Figure \ref{fig:boxplot--dataset--di-spd--exp-full} we observe that
DTs report lower values for both fairness metrics across all
datasets. In Figure \ref{fig:lineplot--frac--corr} we observe that DTs
consistently report lower correlation between DFM and MFM compared to
other models. This indicates that DTs are able to mitigate the bias
present in the training data with a smaller training sample size and
continue to do so as the training same size changes. The above
observation pose an interesting line of query into examining why DTs
are able to produce fairer predictions using explainable AI
techniques.

\section{Threats to Validity}\label{sec:threats}
% we only use two fairness metrics while prior work have used 4. this is because there are currently only 2 fairness metrics with a data variant. this promotes the need for more data-centric fairness metrics

% real world implications are clear, practitioners will not conduct
% this correlation analysis to determine if they have insufficient
% data or not...

% on the matter of simulating distribution change in the training
% data; we validated with no distribution change as well, when there
% is no distribution, the correlation cannot be calculated so the
% analysis fails

We do not apply the \emph{Bonferroni correction} to the correlation
analysis results. Although we report the $pvalue$ for completeness,
we do not base our implications only on the statistically significant
results. But rather on general trends observed in our analysis.

We use the spearman correlation implementation provided by
scipy \cite{virtanen2020scipy} python library. The $pvalue$ provided by
the current implementation however is not reliable for population size
less than 500 experiments\footnote{See notes in the
\href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html}{library
  documentation}}. This is a threat to the experiments conducted in
Section \ref{sec:results-full-rel} and
Section \ref{sec:results-corr-frac} since we only have 50 data points
for each training sample size.

To mitigate this threat, we additionally conduct linear regression
analysis for all experiments using ordinary-least squares. We check
the coefficient of determination ($R^2$) and the mean squared error
(MSE) in the residuals to evaluate the goodness of fit. We set the MFM
as the outcome (or dependent) variable $y$ and the DFM as the design
(or independent) variable $x$. The linear regression results align
with our findings from the correlation analysis.

%% For the experiment in Section \ref{sec:results-full-rel}, the $R^2$ in
%% majority of the cases lie close to 0 when there is no significant
%% change in the distribution of the training set. This indicates that
%% the linear regression model is unable to explain the variability in
%% the MFM using the DFM. With the simulated distribution change in the
%% training set, $R^2$ improves relative to the prior experiment thus
%% reflecting the change we also observe in the correlation analysis.


%% lack of correlation may arise from lack of significant bias in the
%% dataset; thus the DFM and MFM are just random and will never be
%% related; see german-sex for example

For the experiment conducted in Section \ref{sec:results-full-rel},
the simulation of distribution change using a smaller training sample
size of 60\% is a gross approximation and may not be a good fit for
all datasets used in this study. The size of the datasets used in the
study are not consistent. Thus 60\% in large datasets such as
\emph{adult-sex} may be sufficient training data for the ML models to
mitigate some of the bias. An alternative albeit computationally more
expensive solution would be to identify this threshold individually
for each dataset and update it dynamically as its distribution
changes.

\section{Conclusion}\label{sec:conclude}

Prior work in ML fairness testing evaluate fairness after the training
stage, using the predictions of the ML model. In contrast, this study
presents a more holistic approach by testing for fairness at two
distinct locations of the ML development lifecycle. We analyse the
relationship between model dependent and independent fairness metrics
empirically and find a linear relationship between data and model
fairness metrics when the distribution and the size of the training
data changes. Our results indicate that testing for fairness prior to
training can be a ``cheap'' and effective means of catching fairness
issues in the upstream stages of automated ML pipelines and aid
practitioners navigate the complex landscape of fairness testing. As
an extension of this study, we wish to evaluate the effectiveness of
DFM in real-world ML systems.

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
